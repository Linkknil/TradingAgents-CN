#!/usr/bin/env python3
"""
æ··åˆæ£€ç´¢ç¤ºä¾‹æ¼”ç¤º

å±•ç¤ºå‘é‡æ£€ç´¢ã€å…³é”®è¯æ£€ç´¢å’Œæ··åˆæ£€ç´¢çš„åŒºåˆ«å’Œæ•ˆæœã€‚
"""

from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
import numpy as np
from typing import List, Tuple


def create_sample_documents() -> List[Document]:
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ç”¨äºæ¼”ç¤º"""
    documents = [
        Document(
            page_content="è‹¹æœå…¬å¸(AAPL)çš„è‚¡ä»·åœ¨2024å¹´ç¬¬ä¸€å­£åº¦ä¸Šæ¶¨äº†15%ï¼Œä¸»è¦å¾—ç›ŠäºiPhoneé”€é‡å¢é•¿å’ŒAIèŠ¯ç‰‡ä¸šåŠ¡çš„çªç ´ã€‚å…¬å¸CEOè’‚å§†Â·åº“å…‹è¡¨ç¤ºå¯¹æœªæ¥å‘å±•å……æ»¡ä¿¡å¿ƒã€‚",
            metadata={"source": "è´¢ç»æ–°é—»", "date": "2024-03-15"}
        ),
        Document(
            page_content="äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼ŒåŒ…æ‹¬é‡åŒ–äº¤æ˜“ã€é£é™©è¯„ä¼°ã€æ™ºèƒ½æŠ•é¡¾å’Œç®—æ³•äº¤æ˜“ç­‰ã€‚è¿™äº›æŠ€æœ¯æ­£åœ¨æ”¹å˜ä¼ ç»Ÿçš„æŠ•èµ„æ–¹å¼ã€‚",
            metadata={"source": "æŠ€æœ¯æŠ¥å‘Š", "date": "2024-02-20"}
        ),
        Document(
            page_content="AAPLè‚¡ç¥¨ä»£ç å¯¹åº”çš„å…¬å¸æ˜¯è‹¹æœå…¬å¸ï¼Œå…¶å¸‚å€¼è¶…è¿‡3ä¸‡äº¿ç¾å…ƒï¼Œæ˜¯å…¨çƒå¸‚å€¼æœ€é«˜çš„å…¬å¸ä¹‹ä¸€ã€‚è‹¹æœå…¬å¸æ€»éƒ¨ä½äºç¾å›½åŠ åˆ©ç¦å°¼äºšå·åº“æ¯”è’‚è¯ºã€‚",
            metadata={"source": "å…¬å¸ç®€ä»‹", "date": "2024-01-10"}
        ),
        Document(
            page_content="è‚¡ç¥¨æŠ•èµ„éœ€è¦å…³æ³¨å…¬å¸çš„åŸºæœ¬é¢åˆ†æï¼ŒåŒ…æ‹¬è´¢åŠ¡æŒ‡æ ‡ã€è¡Œä¸šåœ°ä½ã€ç®¡ç†å›¢é˜Ÿå’Œæœªæ¥å‘å±•å‰æ™¯ã€‚æŠ€æœ¯åˆ†æä¹Ÿæ˜¯é‡è¦çš„æŠ•èµ„å·¥å…·ã€‚",
            metadata={"source": "æŠ•èµ„æŒ‡å—", "date": "2024-02-05"}
        ),
        Document(
            page_content="è‹¹æœå…¬å¸æœ€æ–°å‘å¸ƒçš„iPhone 15ç³»åˆ—æ‰‹æœºåœ¨å¸‚åœºä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œé”€é‡è¶…å‡ºé¢„æœŸã€‚åŒæ—¶ï¼Œå…¬å¸åœ¨AIå’Œæœºå™¨å­¦ä¹ é¢†åŸŸæŠ•å…¥å·¨å¤§ï¼Œå¼€å‘äº†A17 ProèŠ¯ç‰‡ã€‚",
            metadata={"source": "äº§å“æ–°é—»", "date": "2024-03-20"}
        ),
        Document(
            page_content="é‡åŒ–äº¤æ˜“ä½¿ç”¨æ•°å­¦æ¨¡å‹å’Œç®—æ³•æ¥åˆ†æå¸‚åœºæ•°æ®ï¼Œå¯»æ‰¾æŠ•èµ„æœºä¼šã€‚è¿™ç§æ–¹æ³•åœ¨è‚¡ç¥¨ã€æœŸè´§å’Œå¤–æ±‡å¸‚åœºéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚",
            metadata={"source": "äº¤æ˜“ç­–ç•¥", "date": "2024-01-25"}
        )
    ]
    return documents


def simple_vector_search(query: str, documents: List[Document]) -> List[Tuple[Document, float]]:
    """ç®€å•çš„å‘é‡æœç´¢æ¨¡æ‹Ÿï¼ˆå®é™…é¡¹ç›®ä¸­ä¼šä½¿ç”¨çœŸå®çš„embeddingæ¨¡å‹ï¼‰"""
    # è¿™é‡Œä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…æ¥æ¨¡æ‹Ÿå‘é‡æœç´¢
    query_words = set(query.lower().split())
    
    results = []
    for doc in documents:
        doc_words = set(doc.page_content.lower().split())
        # è®¡ç®—è¯æ±‡é‡å åº¦ä½œä¸ºç›¸ä¼¼åº¦åˆ†æ•°
        overlap = len(query_words & doc_words)
        similarity = overlap / len(query_words) if query_words else 0
        results.append((doc, similarity))
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    results.sort(key=lambda x: x[1], reverse=True)
    return results


def keyword_search(query: str, documents: List[Document]) -> List[Document]:
    """å…³é”®è¯æœç´¢"""
    query_words = set(query.lower().split())
    
    results = []
    for doc in documents:
        doc_text = doc.page_content.lower()
        # è®¡ç®—å…³é”®è¯åŒ¹é…æ•°é‡
        matches = sum(1 for word in query_words if word in doc_text)
        if matches > 0:
            results.append((doc, matches))
    
    # æŒ‰åŒ¹é…æ•°é‡æ’åº
    results.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, score in results]


def hybrid_search(
    query: str, 
    documents: List[Document], 
    vector_weight: float = 0.7, 
    keyword_weight: float = 0.3
) -> List[Document]:
    """æ··åˆæœç´¢"""
    # è·å–å‘é‡æœç´¢ç»“æœ
    vector_results = simple_vector_search(query, documents)
    
    # è·å–å…³é”®è¯æœç´¢ç»“æœ
    keyword_results = keyword_search(query, documents)
    
    # åˆ›å»ºæ–‡æ¡£åˆ°åˆ†æ•°çš„æ˜ å°„
    doc_scores = {}
    
    # å¤„ç†å‘é‡æœç´¢ç»“æœ
    for doc, score in vector_results:
        doc_id = id(doc)
        doc_scores[doc_id] = {
            'doc': doc,
            'vector_score': score,
            'keyword_score': 0.0
        }
    
    # å¤„ç†å…³é”®è¯æœç´¢ç»“æœ
    for i, doc in enumerate(keyword_results):
        doc_id = id(doc)
        keyword_score = 1.0 / (i + 1)  # æ’ååˆ†æ•°
        
        if doc_id in doc_scores:
            doc_scores[doc_id]['keyword_score'] = keyword_score
        else:
            doc_scores[doc_id] = {
                'doc': doc,
                'vector_score': 0.0,
                'keyword_score': keyword_score
            }
    
    # è®¡ç®—ç»¼åˆåˆ†æ•°
    scored_docs = []
    for doc_id, scores in doc_scores.items():
        combined_score = (
            vector_weight * scores['vector_score'] + 
            keyword_weight * scores['keyword_score']
        )
        scored_docs.append((scores['doc'], combined_score))
    
    # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
    scored_docs.sort(key=lambda x: x[1], reverse=True)
    
    return [doc for doc, score in scored_docs]


def demonstrate_retrieval_methods():
    """æ¼”ç¤ºä¸åŒæ£€ç´¢æ–¹æ³•çš„æ•ˆæœ"""
    print("=" * 80)
    print("æ··åˆæ£€ç´¢æ¼”ç¤º")
    print("=" * 80)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    documents = create_sample_documents()
    
    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "è‹¹æœå…¬å¸è‚¡ç¥¨è¡¨ç°å¦‚ä½•ï¼Ÿ",
        "äººå·¥æ™ºèƒ½åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨",
        "AAPLå…¬å¸çš„å¸‚å€¼å’Œæ€»éƒ¨ä½ç½®",
        "é‡åŒ–äº¤æ˜“å’Œç®—æ³•äº¤æ˜“çš„åŒºåˆ«"
    ]
    
    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        print("-" * 60)
        
        # 1. å‘é‡æœç´¢
        print("\nğŸ“Š å‘é‡æœç´¢ç»“æœ:")
        vector_results = simple_vector_search(query, documents)
        for i, (doc, score) in enumerate(vector_results[:3], 1):
            print(f"  {i}. [ç›¸ä¼¼åº¦: {score:.3f}] {doc.page_content[:100]}...")
        
        # 2. å…³é”®è¯æœç´¢
        print("\nğŸ”¤ å…³é”®è¯æœç´¢ç»“æœ:")
        keyword_results = keyword_search(query, documents)
        for i, doc in enumerate(keyword_results[:3], 1):
            print(f"  {i}. {doc.page_content[:100]}...")
        
        # 3. æ··åˆæœç´¢ï¼ˆå¹³è¡¡æƒé‡ï¼‰
        print("\nâš–ï¸ æ··åˆæœç´¢ç»“æœ (å¹³è¡¡æƒé‡ 0.5:0.5):")
        hybrid_results = hybrid_search(query, documents, 0.5, 0.5)
        for i, doc in enumerate(hybrid_results[:3], 1):
            print(f"  {i}. {doc.page_content[:100]}...")
        
        # 4. æ··åˆæœç´¢ï¼ˆå‘é‡æƒé‡é«˜ï¼‰
        print("\nğŸ¯ æ··åˆæœç´¢ç»“æœ (å‘é‡æƒé‡é«˜ 0.8:0.2):")
        vector_heavy_results = hybrid_search(query, documents, 0.8, 0.2)
        for i, doc in enumerate(vector_heavy_results[:3], 1):
            print(f"  {i}. {doc.page_content[:100]}...")
        
        # 5. æ··åˆæœç´¢ï¼ˆå…³é”®è¯æƒé‡é«˜ï¼‰
        print("\nğŸ” æ··åˆæœç´¢ç»“æœ (å…³é”®è¯æƒé‡é«˜ 0.2:0.8):")
        keyword_heavy_results = hybrid_search(query, documents, 0.2, 0.8)
        for i, doc in enumerate(keyword_heavy_results[:3], 1):
            print(f"  {i}. {doc.page_content[:100]}...")


def analyze_retrieval_characteristics():
    """åˆ†æä¸åŒæ£€ç´¢æ–¹æ³•çš„ç‰¹ç‚¹"""
    print("\n" + "=" * 80)
    print("æ£€ç´¢æ–¹æ³•ç‰¹ç‚¹åˆ†æ")
    print("=" * 80)
    
    characteristics = {
        "å‘é‡æ£€ç´¢": {
            "ä¼˜åŠ¿": [
                "ç†è§£è¯­ä¹‰ç›¸ä¼¼æ€§",
                "èƒ½åŒ¹é…åŒä¹‰è¯å’Œè¿‘ä¹‰è¯",
                "é€‚åˆæ¦‚å¿µæ€§æœç´¢",
                "å¯¹æŸ¥è¯¢è¡¨è¾¾æ–¹å¼ä¸æ•æ„Ÿ"
            ],
            "åŠ£åŠ¿": [
                "å¯èƒ½é”™è¿‡ç²¾ç¡®çš„å…³é”®è¯åŒ¹é…",
                "å¯¹ä¸“ä¸šæœ¯è¯­å’Œæ•°å­—ä¸å¤Ÿæ•æ„Ÿ",
                "è®¡ç®—æˆæœ¬è¾ƒé«˜"
            ],
            "é€‚ç”¨åœºæ™¯": [
                "æ¦‚å¿µæ€§æŸ¥è¯¢",
                "åŒä¹‰è¯ä¸°å¯Œçš„é¢†åŸŸ",
                "éœ€è¦è¯­ä¹‰ç†è§£çš„åœºæ™¯"
            ]
        },
        "å…³é”®è¯æ£€ç´¢": {
            "ä¼˜åŠ¿": [
                "ç²¾ç¡®åŒ¹é…ç‰¹å®šæœ¯è¯­",
                "æ£€ç´¢é€Ÿåº¦å¿«",
                "å¯¹æ•°å­—ã€ä»£ç ã€ä¸“ä¸šæœ¯è¯­æ•æ„Ÿ",
                "è®¡ç®—æˆæœ¬ä½"
            ],
            "åŠ£åŠ¿": [
                "æ— æ³•ç†è§£è¯­ä¹‰å…³ç³»",
                "å¯¹æŸ¥è¯¢è¡¨è¾¾æ–¹å¼æ•æ„Ÿ",
                "å¯èƒ½é”™è¿‡ç›¸å…³ä½†ç”¨è¯ä¸åŒçš„å†…å®¹"
            ],
            "é€‚ç”¨åœºæ™¯": [
                "ç²¾ç¡®æœ¯è¯­æŸ¥è¯¢",
                "ä¸“ä¸šé¢†åŸŸæœç´¢",
                "éœ€è¦å¿«é€Ÿå“åº”çš„åœºæ™¯"
            ]
        },
        "æ··åˆæ£€ç´¢": {
            "ä¼˜åŠ¿": [
                "ç»“åˆä¸¤ç§æ–¹æ³•çš„ä¼˜åŠ¿",
                "æé«˜æ£€ç´¢å‡†ç¡®æ€§å’Œå¬å›ç‡",
                "é€‚åº”ä¸åŒç±»å‹çš„æŸ¥è¯¢",
                "å¯è°ƒèŠ‚æƒé‡å¹³è¡¡"
            ],
            "åŠ£åŠ¿": [
                "è®¡ç®—æˆæœ¬è¾ƒé«˜",
                "éœ€è¦è°ƒä¼˜æƒé‡å‚æ•°",
                "å®ç°å¤æ‚åº¦è¾ƒé«˜"
            ],
            "é€‚ç”¨åœºæ™¯": [
                "ç»¼åˆçŸ¥è¯†åº“æœç´¢",
                "éœ€è¦é«˜å‡†ç¡®ç‡çš„åœºæ™¯",
                "æŸ¥è¯¢ç±»å‹å¤šæ ·åŒ–çš„åº”ç”¨"
            ]
        }
    }
    
    for method, info in characteristics.items():
        print(f"\nğŸ“‹ {method}:")
        print(f"  ä¼˜åŠ¿: {', '.join(info['ä¼˜åŠ¿'])}")
        print(f"  åŠ£åŠ¿: {', '.join(info['åŠ£åŠ¿'])}")
        print(f"  é€‚ç”¨åœºæ™¯: {', '.join(info['é€‚ç”¨åœºæ™¯'])}")


def show_implementation_tips():
    """å±•ç¤ºå®ç°å»ºè®®"""
    print("\n" + "=" * 80)
    print("æ··åˆæ£€ç´¢å®ç°å»ºè®®")
    print("=" * 80)
    
    tips = [
        "1. æƒé‡è°ƒä¼˜: æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯è°ƒæ•´å‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢çš„æƒé‡æ¯”ä¾‹",
        "2. æŸ¥è¯¢é¢„å¤„ç†: å¯¹ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œæ¸…æ´—ã€æ‰©å±•å’Œæ ‡å‡†åŒ–",
        "3. ç»“æœå»é‡: é¿å…è¿”å›é‡å¤çš„æ–‡æ¡£ç‰‡æ®µ",
        "4. æ€§èƒ½ä¼˜åŒ–: ä½¿ç”¨ç¼“å­˜å’Œå¼‚æ­¥å¤„ç†æé«˜å“åº”é€Ÿåº¦",
        "5. è¯„ä¼°æŒ‡æ ‡: ä½¿ç”¨MRRã€NDCGç­‰æŒ‡æ ‡è¯„ä¼°æ£€ç´¢æ•ˆæœ",
        "6. åŠ¨æ€æƒé‡: æ ¹æ®æŸ¥è¯¢ç±»å‹åŠ¨æ€è°ƒæ•´æƒé‡æ¯”ä¾‹",
        "7. æŸ¥è¯¢æ‰©å±•: ä½¿ç”¨åŒä¹‰è¯ã€ç›¸å…³è¯æ‰©å±•æŸ¥è¯¢",
        "8. ç»“æœé‡æ’åº: åŸºäºé¢å¤–ä¿¡å·å¯¹ç»“æœè¿›è¡Œé‡æ–°æ’åº"
    ]
    
    for tip in tips:
        print(f"  {tip}")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demonstrate_retrieval_methods()
    analyze_retrieval_characteristics()
    show_implementation_tips()
    
    print("\n" + "=" * 80)
    print("æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)

